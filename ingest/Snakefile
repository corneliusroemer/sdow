DOWNLOAD_DATE = "20231220"

DOWNLOAD_URL = (
    f"https://mirror.accum.se/mirror/wikimedia.org/dumps/enwiki/{DOWNLOAD_DATE}"
)

SHA1SUM_FILENAME = f"enwiki-{DOWNLOAD_DATE}-sha1sums.txt"
REDIRECTS_FILENAME = f"enwiki-{DOWNLOAD_DATE}-redirect.sql.gz"
PAGES_FILENAME = f"enwiki-{DOWNLOAD_DATE}-page.sql.gz"
LINKS_FILENAME = f"enwiki-{DOWNLOAD_DATE}-pagelinks.sql.gz"


rule all:
    input:
        "work/sdow.sqlite.gz",


rule download_sha1sums:
    output:
        sha1sum="data/sha1sum.txt",
    shell:
        """
        wget -O {output.sha1sum} {DOWNLOAD_URL}/{SHA1SUM_FILENAME}
        """


rule download:
    input:
        sha1sum="data/sha1sum.txt",
    output:
        file="data/{filename}",
    shell:
        """
        cd data
        wget -O {wildcards.filename} {DOWNLOAD_URL}/{wildcards.filename}
        grep {wildcards.filename} sha1sum.txt | sha1sum -c
        """


rule download_all:
    input:
        expand(
            "data/{filename}",
            filename=[
                REDIRECTS_FILENAME,
                PAGES_FILENAME,
                LINKS_FILENAME,
            ],
        ),


rule trim_redirects:
    """
    Output: tsv file with columns:
        redirect_id (int)
        redirect_title (not quoted, but escaped with _ for spaces and \' for ')
    """
    input:
        f"data/{REDIRECTS_FILENAME}",
    output:
        f"work/redirects.tsv.zst",
    shell:
        r"""
        pigz -dc {input} \
        | wikiparse redirect 2 \
        | zstd -c > {output}
        """


rule trim_pages:
    """
    Output: tsv file with columns:
        page_id (int)
        page_title (not quoted, but escaped with _ for spaces and \' for ')
        page_is_redirect (0 or 1)
    """
    input:
        f"data/{PAGES_FILENAME}",
    output:
        f"work/pages.tsv.zst",
    shell:
        r"""
        pigz -dc {input} \
        | wikiparse page 5 \
        | zstd -c > {output}
        """


rule trim_links:
    """
    Output: tsv file with columns:
        source_id (int)
        target_title (not quoted, but escaped with _ for spaces and \' for ')
    """
    input:
        f"data/{LINKS_FILENAME}",
    output:
        f"work/links.tsv.zst",
    shell:
        r"""
        pigz -dc {input} \
        | wikiparse pagelinks 5 \
        | zstd -c > {output}
        """


rule replace_titles_in_redirects_file:
    input:
        pages="work/pages.tsv.zst",
        redirects="work/redirects.tsv.zst",
    output:
        "work/redirects.with_ids.tsv.zst",
    shell:
        r"""
        export LC_ALL=C
        python ../scripts/replace_titles_in_redirects_file.py {input.pages} {input.redirects} \
        | parsort -S 5% -t $'\t' -k 1n,1n \
        | zstd -c > {output}
        """


rule replace_titles_and_redirects_in_links_file:
    input:
        pages="work/pages.tsv.zst",
        redirects="work/redirects.with_ids.tsv.zst",
        links="work/links.tsv.zst",
    output:
        "work/links.with_ids.tsv.zst",
    shell:
        r"""
        python ../scripts/replace_titles_and_redirects_in_links_file.py {input.pages} {input.redirects} {input.links} \
        | zstd -c > {output}
        """


rule prune_pages_file:
    input:
        pages="work/pages.tsv.zst",
        redirects="work/redirects.with_ids.tsv.zst",
    output:
        "work/pages.pruned.tsv.zst",
    shell:
        r"""
        python ../scripts/prune_pages_file.py {input.pages} {input.redirects} \
        | zstd -c > {output}
        """


rule sort_links_by_source_id:
    input:
        "work/links.with_ids.tsv.zst",
    output:
        "work/links.sorted_by_source_id.tsv.zst",
    threads: 6
    shell:
        r"""
        export LC_ALL=C
        zstdcat {input} \
        | parsort -S 5% -t $'\t' -k 1n,1n \
        | uniq \
        | zstd -c > {output}
        """


rule sort_links_by_target_id:
    input:
        "work/links.with_ids.tsv.zst",
    output:
        "work/links.sorted_by_target_id.tsv.zst",
    threads: 6
    shell:
        r"""
        export LC_ALL=C
        zstdcat {input} \
        | parsort -S 5% -t $'\t' -k 2n,2n \
        | uniq \
        | zstd -c > {output}
        """


rule group_links_by_source_id:
    input:
        "work/links.sorted_by_source_id.tsv.zst",
    output:
        "work/links.grouped_by_source_id.tsv.zst",
    shell:
        r"""
        zstdcat {input} \
        | awk -F '\t' '$1==last {{printf "|%s",$2; next}} NR>1 {{print "";}} {{last=$1; printf "%s\t%s",$1,$2;}} END{{print "";}}' \
        | zstd -c > {output}
        """


rule group_links_by_target_id:
    input:
        "work/links.sorted_by_target_id.tsv.zst",
    output:
        "work/links.grouped_by_target_id.tsv.zst",
    shell:
        r"""
        zstdcat {input} \
        | awk -F '\t' '$2==last {{printf "|%s",$1; next}} NR>1 {{print "";}} {{last=$2; printf "%s\t%s",$2,$1;}} END{{print "";}}' \
        | zstd -c > {output}
        """


rule combine_grouped_links_files:
    input:
        source="work/links.grouped_by_source_id.tsv.zst",
        target="work/links.grouped_by_target_id.tsv.zst",
    output:
        "work/links.with_counts.tsv.zst",
    shell:
        r"""
        python ../scripts/combine_grouped_links_files.py {input.source} {input.target} \
        | zstd -c > {output}
        """


rule create_sqlite_database:
    input:
        redirects="work/redirects.with_ids.tsv.zst",
        pages="work/pages.pruned.tsv.zst",
        links="work/links.with_counts.tsv.zst",
    output:
        uncompressed="work/sdow.sqlite",
        compressed="work/sdow.sqlite.gz",
    shell:
        r"""
        zstdcat {input.redirects} | sqlite3 {output.uncompressed} ".read ../sql/createRedirectsTable.sql"
        zstdcat {input.pages} | sqlite3 {output.uncompressed} ".read ../sql/createPagesTable.sql"
        zstdcat {input.links} | sqlite3 {output.uncompressed} ".read ../sql/createLinksTable.sql"
        pigz --best --keep {output.uncompressed}
        """
